{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFYRgJFD5YgPC4EXgDp199",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STUPIDTREE/transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer实现"
      ],
      "metadata": {
        "id": "mibh6a1DZOf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import warnings\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.init import xavier_normal_\n",
        "from torch.nn.init import constant_\n"
      ],
      "metadata": {
        "id": "9FeQJatOZVlz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 词嵌入和位置编码\n",
        "\n",
        "\n",
        "1.   Embeddings实现了把输入的语句转换为嵌入表示\n",
        "2.   PositionalEncoding实现了三角函数位置编码\n",
        "\n"
      ],
      "metadata": {
        "id": "LFZm3KYiemKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super().__init__()\n",
        "    self.lut = nn.Embedding(vocab, d_model)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "8zTb6pNnHLWF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositonalEncoding(nn.Module):\n",
        "  def __init__(self, max_len=5000, d_model=512, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    P = torch.zeros(1, max_len, d_model)\n",
        "    pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "    i = torch.arange(0, d_model, 2)\n",
        "    div_term = torch.exp(i * -(math.log(10000.0) / d_model))\n",
        "    P[0, :, 0::2] = torch.sin(pos * div_term)\n",
        "    P[0, :, 1::2] = torch.cos(pos * div_term)\n",
        "    # self.P = Parameter(P, requires_grad=False) # 这种写法是作为模型参数但不参与梯度计算，即不变参数\n",
        "    # 这种写法不出现在state_dict中，做为普通常数张量\n",
        "    self.register_buffer('P', P)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 默认x形状为 (batch, seq_len, d_model)\n",
        "    if x.dim() == 2:  # 说明缺少 batch 维度\n",
        "      x = x.unsqueeze(0)  # 变成 (1, seq_len, d_model)\n",
        "    # x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) #Variable为旧版本写法，Variable 现在已经合并到 Tensor，可以直接写成x.detach()\n",
        "    x = x + self.P[:, :x.size(1), :]\n",
        "    return self.dropout(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "9x6A7Uyoep0M"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "测试"
      ],
      "metadata": {
        "id": "4VP0WxQEibjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((2, 4), dtype=torch.long)\n",
        "emb_layer = Embeddings(8, 10)\n",
        "emb = emb_layer(x)\n",
        "print(emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqOUr4V1a7VL",
        "outputId": "fa4e2bed-9a3b-469d-ee3c-aab594657385"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pe_layer = PositonalEncoding(10, 8)\n",
        "x = pe_layer(emb)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZodthZnFAxA",
        "outputId": "63d5175d-f518-4565-de05-9659c3061753"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 编码器实现\n",
        "\n",
        "\n",
        "\n",
        "1.   Encoder实现了编码器的多层结构，论文实现为6层；\n",
        "2.   EncoderLayer实现了编码器每一层内部的两层连接，包含一个MHA层，及一个FFN层；\n",
        "3.   SublayerConnection实现了MHA层和FFN层的子功能层+add&norm；\n",
        "4.   MultiHeadAttention是MHA的实现，调用了attention函数，attention实现了QKV的矩阵计算；\n",
        "5.   PositionwiseFeedForward实现了FFN全连接。\n",
        "\n"
      ],
      "metadata": {
        "id": "3GuH0klZi0zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  d_k = query.size(-1) # 对应词嵌入维度？\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "  p_attn = F.softmax(scores, dim = -1)\n",
        "\n",
        "  if dropout is not None:\n",
        "    # 这样写对吗？\n",
        "    p_attn = dropout(p_attn)\n",
        "\n",
        "  return torch.matmul(p_attn, value), p_attn\n"
      ],
      "metadata": {
        "id": "a45s-JkXiy9a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, d_model, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout = dropout\n",
        "\n",
        "    assert d_model % num_heads == 0\n",
        "    self.d_k = d_model // num_heads\n",
        "    self.h = num_heads\n",
        "\n",
        "    self.linears = nn.ModuleList(nn.Linear(d_model, d_model) for _ in range(4))\n",
        "    self.attn = None\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(1) # 代表多头中的第n头？\n",
        "\n",
        "    num_batches = query.size(0)\n",
        "    # q, k, v经过一次线性变换，重新调整形状，拆分为h个d_k维度的小块(多头注意力实现，和self-attention一样的算力，但效果更好),\n",
        "    # [batch, seq_len, d_model] -> [batch, seq_len, h, d_k]\n",
        "    query, key, value = [l(x).view(num_batches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "    x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "    # [batch, seq_len, h, d_k] -> [batch, seq_len, d_model]\n",
        "    x = x.transpose(1, 2).contiguous().view(num_batches, -1, self.h * self.d_k)\n",
        "\n",
        "    return self.linears[-1](x)\n"
      ],
      "metadata": {
        "id": "8OM3dsErMvFH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha_layer = MultiHeadAttention(8, 512)\n",
        "q,k,v = torch.rand((2, 4, 512)), torch.rand((2, 4, 512)), torch.rand((2, 4, 512))\n",
        "x = mha_layer(q, k, v)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqPrQf-l7uFF",
        "outputId": "87c8ebcc-e6c1-4463-dbea-55fe9857ac3b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "5KCzy2td_214"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "      super().__init__()\n",
        "      self.layers = clones(layer, N)\n",
        "      self.norm = LayerNorm(layer.d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "      for layer in self.layers:\n",
        "        x = layer(x,mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "P2vuICOXqzwB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self, size, dropout):\n",
        "    super().__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    sublayer_out = sublayer(x)\n",
        "    return self.norm(x + self.dropout(sublayer_out))\n"
      ],
      "metadata": {
        "id": "-Q-_tevmsK8d"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, self_attn, feed_forward, dropout):\n",
        "    super().__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    z = self.sublayer[1](x, self.feed_forward)\n",
        "    return z"
      ],
      "metadata": {
        "id": "eo32-9zNuSf9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.w1 =nn.Linear(d_model, d_ff)\n",
        "    self.w2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w2(self.dropout(F.relu(self.w1(x))))"
      ],
      "metadata": {
        "id": "9VUGM0ay3n6a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "自定义LayerNorm函数，gamma和beta是可训练的参数。\n",
        "如果你只是想用标准的层归一化，建议直接用 nn.LayerNorm。但如果你需要：\n",
        "\n",
        "自定义 gamma 和 beta 的初始化\n",
        "在 forward 中进行其他特殊操作\n",
        "使用不同的归一化策略（如批量归一化或组归一化）\n",
        "那么自定义 LayerNorm 可能更合适。"
      ],
      "metadata": {
        "id": "Z4TSI1LlrNXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, feature_size, eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # 这里需要重新理解\n",
        "    self.gamma = nn.Parameter(torch.ones(feature_size))\n",
        "    self.beta = nn.Parameter(torch.zeros(feature_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.gamma * (x - mean) / (std + self.eps) + self.beta\n"
      ],
      "metadata": {
        "id": "Gsgy74Yw6d58"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "  attn_shape = (1, size, size)\n",
        "  subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "\n",
        "  return torch.from_numpy(subsequent_mask) == 0"
      ],
      "metadata": {
        "id": "ToeQrgVpMTE8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 解码器实现\n",
        "解码器部分结构：\n",
        "\n",
        "\n",
        "1.   Decoder,实现了解码器的6层组成结构；\n",
        "2.   DecoderLayer，实现了每一个解码器层的实现，由三部分组成，自注意力mask-MHA，交叉注意力MHA，及全连接层FFN；\n",
        "3.   Geneator, 解码器最终的输出层，包含一个线性映射层和一个softmax层。"
      ],
      "metadata": {
        "id": "CLrLw0hQX1YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super().__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.d_model)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "GqpLHU4TXyXo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, self_attn, src_attn, feed_forward, dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(d_model, dropout), 3)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
        "\n",
        "    return self.sublayer[2](x, self.feed_forward)\n"
      ],
      "metadata": {
        "id": "-U7qfbAMPtSs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "obBH9doDUV78"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型构造"
      ],
      "metadata": {
        "id": "bLiZkUFDc0BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, x, tgt, src_mask, tgt_mask):\n",
        "    memory = self.encoder(self.src_embed(x), src_mask)\n",
        "    x = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "    return self.generator(x)\n"
      ],
      "metadata": {
        "id": "T1mgq24Wc9ta"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "  dc = copy.deepcopy\n",
        "  attn = MultiHeadAttention(h, d_model)\n",
        "  ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "  pos = PositonalEncoding(d_model=d_model, dropout=dropout)\n",
        "  encoder = Encoder(EncoderLayer(d_model, dc(attn), dc(ff), dropout), N)\n",
        "  decoder = Decoder(DecoderLayer(d_model, dc(attn), dc(attn), dc(ff), dropout), N)\n",
        "  src_embed = nn.Sequential(Embeddings(d_model, src_vocab), dc(pos))\n",
        "  tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), dc(pos))\n",
        "  generator = Generator(d_model, tgt_vocab)\n",
        "  model = EncoderDecoder(\n",
        "      encoder, decoder, src_embed, tgt_embed, generator\n",
        "  )\n",
        "\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "KaBuFSh4fpe1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据集生成"
      ],
      "metadata": {
        "id": "DFYi2Zkyp2_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]    # decoder的输入（即期望输出除了最后一个token以外的部分)\n",
        "            self.trg_y = trg[:, 1:]   # decoder的期望输出（trg基础上再删去句子起始符）\n",
        "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "\n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"\"\"\n",
        "        Create a mask to hide padding and future words.\n",
        "        padd 和 future words 均在mask中用0表示\n",
        "        \"\"\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
        "        return tgt_mask\n",
        "\n",
        "\n",
        "# Synthetic Data\n",
        "def data_gen(V, slen, batch, nbatches, device):\n",
        "    \"\"\"\n",
        "    Generate random data for a src-tgt copy task.\n",
        "    V: 词典数量，取值范围[0, V-1]，约定0作为特殊符号使用代表padding\n",
        "    slen: 生成的序列数据的长度\n",
        "    batch: batch_size\n",
        "    nbatches: number of batches to generate\n",
        "    \"\"\"\n",
        "    for i in range(nbatches):\n",
        "\n",
        "        data = torch.from_numpy(np.random.randint(2, V, size=(batch, slen)))\n",
        "        # 约定输出为输入除去序列第一个元素，即向后平移一位进行输出，同时输出数据要在第一个时间步添加一个起始符\n",
        "        # 因此，加入输入数据为  [3, 4, 2, 6, 4, 5]\n",
        "        # ground truth输出为 [1, 4, 2, 6, 4, 5]\n",
        "        tgt_data = data.clone()\n",
        "        tgt_data[:, 0] = 1   # 将序列的第一个时间步置为1(即约定的起始符)，即可完成GT数据的构造\n",
        "        src = data.clone().detach().to(data.device)\n",
        "        tgt = tgt_data.clone().detach().to(data.device)\n",
        "        if device == \"cuda\":\n",
        "            src = src.cuda()\n",
        "            tgt = tgt.cuda()\n",
        "        yield Batch(src, tgt, 0)\n",
        "\n",
        "\n",
        "# test data_gen\n",
        "data_iter = data_gen(V=5, slen=10, batch=2, nbatches=10, device=\"cpu\")\n",
        "for i, batch in enumerate(data_iter):\n",
        "    print(\"\\nbatch.src\")\n",
        "    print(batch.src.shape)\n",
        "    print(batch.src)\n",
        "    print(\"\\nbatch.trg\")\n",
        "    print(batch.trg.shape)\n",
        "    print(batch.trg)\n",
        "    print(\"\\nbatch.trg_y\")\n",
        "    print(batch.trg_y.shape)\n",
        "    print(batch.trg_y)\n",
        "    print(\"\\nbatch.src_mask\")\n",
        "    print(batch.src_mask.shape)\n",
        "    print(batch.src_mask)\n",
        "    print(\"\\nbatch.trg_mask\")\n",
        "    print(batch.trg_mask.shape)\n",
        "    print(batch.trg_mask)\n",
        "    break\n",
        "#raise RuntimeError()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69yxDeMbqqBV",
        "outputId": "764e1730-4e4c-4688-aeca-8eac112bcbf9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch.src\n",
            "torch.Size([2, 10])\n",
            "tensor([[2, 3, 4, 2, 4, 3, 3, 4, 4, 2],\n",
            "        [3, 2, 4, 2, 3, 3, 3, 4, 4, 4]])\n",
            "\n",
            "batch.trg\n",
            "torch.Size([2, 9])\n",
            "tensor([[1, 3, 4, 2, 4, 3, 3, 4, 4],\n",
            "        [1, 2, 4, 2, 3, 3, 3, 4, 4]])\n",
            "\n",
            "batch.trg_y\n",
            "torch.Size([2, 9])\n",
            "tensor([[3, 4, 2, 4, 3, 3, 4, 4, 2],\n",
            "        [2, 4, 2, 3, 3, 3, 4, 4, 4]])\n",
            "\n",
            "batch.src_mask\n",
            "torch.Size([2, 1, 10])\n",
            "tensor([[[True, True, True, True, True, True, True, True, True, True]],\n",
            "\n",
            "        [[True, True, True, True, True, True, True, True, True, True]]])\n",
            "\n",
            "batch.trg_mask\n",
            "torch.Size([2, 9, 9])\n",
            "tensor([[[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 训练"
      ],
      "metadata": {
        "id": "MwxZQeS_BOFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(data_iter, model, loss_compute, device=None):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg,\n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        # print(\"out.shape = \", out.shape)\n",
        "        # print(\"batch.trg_y.shape = \", batch.trg_y.shape)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, true_dist.detach())\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        \"\"\"\n",
        "        norm: loss的归一化系数，用batch中所有有效token数即可\n",
        "        \"\"\"\n",
        "        # print(\"x.shape = \", x.shape)\n",
        "        # print(\"y.shape = \", y.shape)\n",
        "        # x = self.generator(x)\n",
        "        x_ = x.contiguous().view(-1, x.size(-1))\n",
        "        y_ = y.contiguous().view(-1)\n",
        "        loss = self.criterion(x_, y_)\n",
        "        loss /= norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        return loss.item() * norm"
      ],
      "metadata": {
        "id": "FTMulswYr5Tn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# -----------------------------------\n",
        "# A Easy Example\n",
        "# -----------------------------------\n",
        "# Train the simple copy task.\n",
        "device = \"cuda\"\n",
        "nrof_epochs = 40 # 40\n",
        "batch_size = 32 # 32\n",
        "V = 11    # 词典的数量\n",
        "sequence_len = 15  # 生成的序列数据的长度\n",
        "nrof_batch_train_epoch = 30    # 训练时每个epoch多少个batch\n",
        "nrof_batch_valid_epoch = 10    # 验证时每个epoch多少个batch\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, V, N=2)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "if device == \"cuda\":\n",
        "    model.cuda()\n",
        "\n",
        "for epoch in range(nrof_epochs):\n",
        "    print(f\"\\nepoch {epoch}\")\n",
        "    print(\"train...\")\n",
        "    model.train()\n",
        "    data_iter = data_gen(V, sequence_len, batch_size, nrof_batch_train_epoch, device)\n",
        "    loss_compute = SimpleLossCompute(model.generator, criterion, optimizer)\n",
        "    train_mean_loss = run_epoch(data_iter, model, loss_compute, device)\n",
        "    print(\"valid...\")\n",
        "    model.eval()\n",
        "    valid_data_iter = data_gen(V, sequence_len, batch_size, nrof_batch_valid_epoch, device)\n",
        "    valid_loss_compute = SimpleLossCompute(model.generator, criterion, None)\n",
        "    valid_mean_loss = run_epoch(valid_data_iter, model, valid_loss_compute, device)\n",
        "    print(f\"valid loss: {valid_mean_loss}\")\n",
        "\n",
        "\n",
        "# greedy decode\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    embed = model.src_embed(src)\n",
        "    memory = model.encoder(embed, src_mask)\n",
        "    # ys代表目前已生成的序列，最初为仅包含一个起始符的序列，不断将预测结果追加到序列最后\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        ys_embed = model.tgt_embed(ys)\n",
        "        out = model.decoder(ys_embed, memory, src_mask,\n",
        "                           subsequent_mask(ys.size(1)).type_as(src.data))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        # next_word = next_word.item()\n",
        "        # ys = torch.cat([ys, torch.ones(1, 1, dtype=torch.long, device=src.device).fill_(next_word)], dim=1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys\n",
        "\n",
        "print(\"greedy decode\")\n",
        "model.eval()\n",
        "src = torch.LongTensor([1, 2, 3, 4, 4, 6, 7, 8, 10, 10]).cuda()\n",
        "src_mask = torch.ones(1, 1, 10).cuda()\n",
        "pred_result = greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)\n",
        "print(pred_result[:, 1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnekrFvJBeMk",
        "outputId": "7f62c97b-655f-467c-cc6e-04ddcac93983"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.583433 Tokens per Sec: 8827.085938\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.940773 Tokens per Sec: 11004.223633\n",
            "valid loss: 1.9794069528579712\n",
            "\n",
            "epoch 1\n",
            "train...\n",
            "Epoch Step: 1 Loss: 3.508346 Tokens per Sec: 11272.271484\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.257387 Tokens per Sec: 12345.225586\n",
            "valid loss: 2.215728998184204\n",
            "\n",
            "epoch 2\n",
            "train...\n",
            "Epoch Step: 1 Loss: 3.143415 Tokens per Sec: 8437.614258\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.389766 Tokens per Sec: 9935.535156\n",
            "valid loss: 2.458388328552246\n",
            "\n",
            "epoch 3\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.960128 Tokens per Sec: 9040.274414\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.104912 Tokens per Sec: 11155.229492\n",
            "valid loss: 2.0838000774383545\n",
            "\n",
            "epoch 4\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.663283 Tokens per Sec: 10862.366211\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.073424 Tokens per Sec: 12312.019531\n",
            "valid loss: 2.0655457973480225\n",
            "\n",
            "epoch 5\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.663819 Tokens per Sec: 11064.809570\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.392645 Tokens per Sec: 12363.500977\n",
            "valid loss: 2.3322677612304688\n",
            "\n",
            "epoch 6\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.583634 Tokens per Sec: 10806.021484\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.172701 Tokens per Sec: 12424.157227\n",
            "valid loss: 2.1701929569244385\n",
            "\n",
            "epoch 7\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.366633 Tokens per Sec: 11006.415039\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.961593 Tokens per Sec: 12187.682617\n",
            "valid loss: 1.9781136512756348\n",
            "\n",
            "epoch 8\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.113349 Tokens per Sec: 11361.591797\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.830606 Tokens per Sec: 12443.286133\n",
            "valid loss: 1.839597225189209\n",
            "\n",
            "epoch 9\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.895583 Tokens per Sec: 9984.820312\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.725262 Tokens per Sec: 10299.088867\n",
            "valid loss: 1.7087198495864868\n",
            "\n",
            "epoch 10\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.793073 Tokens per Sec: 8645.642578\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.574481 Tokens per Sec: 11787.333008\n",
            "valid loss: 1.548467755317688\n",
            "\n",
            "epoch 11\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.657929 Tokens per Sec: 10884.703125\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.388748 Tokens per Sec: 12528.826172\n",
            "valid loss: 1.3866316080093384\n",
            "\n",
            "epoch 12\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.611224 Tokens per Sec: 10876.041016\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.271693 Tokens per Sec: 12784.250000\n",
            "valid loss: 1.278573989868164\n",
            "\n",
            "epoch 13\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.582562 Tokens per Sec: 10865.444336\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.202985 Tokens per Sec: 12259.806641\n",
            "valid loss: 1.2243379354476929\n",
            "\n",
            "epoch 14\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.517418 Tokens per Sec: 10264.685547\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.091402 Tokens per Sec: 12581.170898\n",
            "valid loss: 1.0697107315063477\n",
            "\n",
            "epoch 15\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.369615 Tokens per Sec: 11155.261719\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.792887 Tokens per Sec: 12475.464844\n",
            "valid loss: 0.7842314839363098\n",
            "\n",
            "epoch 16\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.208591 Tokens per Sec: 10193.272461\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.628696 Tokens per Sec: 11483.588867\n",
            "valid loss: 0.6219254732131958\n",
            "\n",
            "epoch 17\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.057517 Tokens per Sec: 8908.016602\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.508338 Tokens per Sec: 7875.076172\n",
            "valid loss: 0.5090650320053101\n",
            "\n",
            "epoch 18\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.909206 Tokens per Sec: 10488.510742\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.298965 Tokens per Sec: 12044.524414\n",
            "valid loss: 0.3116251528263092\n",
            "\n",
            "epoch 19\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.792236 Tokens per Sec: 11047.018555\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.225924 Tokens per Sec: 11347.869141\n",
            "valid loss: 0.2231721132993698\n",
            "\n",
            "epoch 20\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.624319 Tokens per Sec: 10944.902344\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.137168 Tokens per Sec: 12304.401367\n",
            "valid loss: 0.13909859955310822\n",
            "\n",
            "epoch 21\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.341592 Tokens per Sec: 10111.624023\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.091561 Tokens per Sec: 12555.405273\n",
            "valid loss: 0.08563275635242462\n",
            "\n",
            "epoch 22\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.288557 Tokens per Sec: 10895.464844\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.081538 Tokens per Sec: 12600.998047\n",
            "valid loss: 0.06967588514089584\n",
            "\n",
            "epoch 23\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.229497 Tokens per Sec: 11151.985352\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.052066 Tokens per Sec: 11177.292969\n",
            "valid loss: 0.04284738749265671\n",
            "\n",
            "epoch 24\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.197924 Tokens per Sec: 11374.039062\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.012642 Tokens per Sec: 10908.748047\n",
            "valid loss: 0.023799441754817963\n",
            "\n",
            "epoch 25\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.157984 Tokens per Sec: 7365.205566\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.011135 Tokens per Sec: 12410.863281\n",
            "valid loss: 0.012810089625418186\n",
            "\n",
            "epoch 26\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.145641 Tokens per Sec: 11203.082031\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.014612 Tokens per Sec: 12385.341797\n",
            "valid loss: 0.006604604423046112\n",
            "\n",
            "epoch 27\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.137601 Tokens per Sec: 11321.375000\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.006166 Tokens per Sec: 12604.843750\n",
            "valid loss: 0.007642798591405153\n",
            "\n",
            "epoch 28\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.121147 Tokens per Sec: 11005.383789\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.002740 Tokens per Sec: 12271.415039\n",
            "valid loss: 0.003913956228643656\n",
            "\n",
            "epoch 29\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.065993 Tokens per Sec: 11059.763672\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.001211 Tokens per Sec: 12842.353516\n",
            "valid loss: 0.003165516536682844\n",
            "\n",
            "epoch 30\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043722 Tokens per Sec: 10980.207031\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.006619 Tokens per Sec: 12198.007812\n",
            "valid loss: 0.003635809291154146\n",
            "\n",
            "epoch 31\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.055920 Tokens per Sec: 11126.759766\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.000803 Tokens per Sec: 10632.670898\n",
            "valid loss: 0.0011463066330179572\n",
            "\n",
            "epoch 32\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.061235 Tokens per Sec: 9127.574219\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.001808 Tokens per Sec: 11777.875977\n",
            "valid loss: 0.0015215862076729536\n",
            "\n",
            "epoch 33\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.067454 Tokens per Sec: 10671.892578\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.003790 Tokens per Sec: 12017.563477\n",
            "valid loss: 0.002080420032143593\n",
            "\n",
            "epoch 34\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.071112 Tokens per Sec: 10550.375000\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.000334 Tokens per Sec: 12522.688477\n",
            "valid loss: 0.0010910126147791743\n",
            "\n",
            "epoch 35\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043619 Tokens per Sec: 11358.330078\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.002278 Tokens per Sec: 12576.959961\n",
            "valid loss: 0.003199242288246751\n",
            "\n",
            "epoch 36\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.031252 Tokens per Sec: 11137.741211\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.001766 Tokens per Sec: 12438.549805\n",
            "valid loss: 0.0008802711963653564\n",
            "\n",
            "epoch 37\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.032163 Tokens per Sec: 10909.951172\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.000163 Tokens per Sec: 12638.926758\n",
            "valid loss: 0.00045242885244078934\n",
            "\n",
            "epoch 38\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.020843 Tokens per Sec: 10779.953125\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.001146 Tokens per Sec: 12640.117188\n",
            "valid loss: 0.0014870110899209976\n",
            "\n",
            "epoch 39\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043436 Tokens per Sec: 8516.681641\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.000732 Tokens per Sec: 8613.955078\n",
            "valid loss: 0.0016476138262078166\n",
            "greedy decode\n",
            "tensor([[ 2,  3,  4,  4,  6,  7,  8, 10, 10]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}